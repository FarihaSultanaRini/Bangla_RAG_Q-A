{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mczaib-qcME5",
    "outputId": "5d308ba9-47b6-486e-eb63-474637710702"
   },
   "outputs": [],
   "source": [
    "pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C-EywVDdLGp-",
    "outputId": "f2b6a518-df6f-4f29-f1a1-80e5f7bbe369"
   },
   "outputs": [],
   "source": [
    "pip install -U langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vd7fIoojLUML",
    "outputId": "9609293d-a90f-4aa3-f1ae-a917eb4da808"
   },
   "outputs": [],
   "source": [
    "!pip install pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l6uwz3ZyLXKn",
    "outputId": "a211cf9a-a708-419e-ec8e-9df23b5e8780"
   },
   "outputs": [],
   "source": [
    "pip install pdf2image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mdHOXD2MLaOx",
    "outputId": "ebb29d1a-e150-4764-b224-6516ca720a5f"
   },
   "outputs": [],
   "source": [
    "!apt-get install -y poppler-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i37gNCDULdRW",
    "outputId": "16812ed8-376a-4a02-bfdd-83ea069c36f9"
   },
   "outputs": [],
   "source": [
    "!apt-get install -y tesseract-ocr-ben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ubumSYwLfeY",
    "outputId": "a128eb94-eb47-43bf-845e-6f26c7170459"
   },
   "outputs": [],
   "source": [
    "!ls /usr/share/tesseract-ocr/4.00/tessdata/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4HeRDhc0MV9U",
    "outputId": "18fba405-1ab4-43a8-f6c8-581fbf5e1d5d"
   },
   "outputs": [],
   "source": [
    "# Clean up: remove old/broken fitz traces if any\n",
    "!rm -rf /usr/local/lib/python3.11/dist-packages/fitz*\n",
    "\n",
    "# Force reinstall PyMuPDF\n",
    "!pip install --force-reinstall PyMuPDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xg996LVPMiDz",
    "outputId": "a36a8034-3890-41bf-9057-7429be4e7b0c"
   },
   "outputs": [],
   "source": [
    "pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZoFITilMNV1j",
    "outputId": "455025c8-96af-4905-c51d-9f5b559f7484"
   },
   "outputs": [],
   "source": [
    "pip  install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BkqXvPfWLFyH"
   },
   "outputs": [],
   "source": [
    "from langdetect import detect, DetectorFactory\n",
    "import fitz  # PyMuPDF\n",
    "import cv2\n",
    "import pytesseract\n",
    "import numpy as np\n",
    "import pdfplumber\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import faiss\n",
    "from typing import List, Dict, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bvBtxoFyLjHX"
   },
   "outputs": [],
   "source": [
    "pdf_file = '/content/HSC26-Bangla1st-Paper.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PwS3fU5LNO09"
   },
   "outputs": [],
   "source": [
    "\n",
    "def fix_roman_numerals(lines):\n",
    "    \"\"\"\n",
    "    Fix OCR artifacts for Roman numerals.\n",
    "    - Only replace double/triple danda (।।, ।।।) and bars (||).\n",
    "    - Never convert a single danda to 'i'.\n",
    "    - Remove '।' after ক/খ/গ/ঘ (option markers).\n",
    "    - Fix split Bangla characters.\n",
    "    \"\"\"\n",
    "    replacements = {\n",
    "        \"।।।\": \"iii\",\n",
    "        \"।।\": \"ii\",\n",
    "        \"||\": \"ii\",\n",
    "        \"|\": \"i\"\n",
    "    }\n",
    "\n",
    "    fixed_lines = []\n",
    "    for line in lines:\n",
    "        fixed = line\n",
    "\n",
    "        # Remove danda if it's just a separator after ক/খ/গ/ঘ\n",
    "        fixed = re.sub(r'^([কখগঘ])।', r'\\1', fixed)\n",
    "\n",
    "        # Replace numeral artifacts (but not single danda)\n",
    "        for wrong, right in replacements.items():\n",
    "            fixed = fixed.replace(wrong, right)\n",
    "\n",
    "        # Fix Bangla syllable splits (ক া → কা)\n",
    "        fixed = re.sub(r'([ক-হ])\\s([ািীুূৃেৈোৌ])', r'\\1\\2', fixed)\n",
    "\n",
    "        # Normalize spaces and commas\n",
    "        fixed = re.sub(r'\\s*,\\s*', ', ', fixed)\n",
    "        fixed = re.sub(r'\\s+', ' ', fixed).strip()\n",
    "\n",
    "        fixed_lines.append(fixed)\n",
    "\n",
    "    return fixed_lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "umTNO52m66F7"
   },
   "outputs": [],
   "source": [
    "def extract_lines_by_line(text):\n",
    "    \"\"\"\n",
    "    Return the text as a list of lines, preserving everything exactly.\n",
    "    No language detection or filtering — just splits by line.\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    for line in text.splitlines():\n",
    "        clean_line = line.strip()\n",
    "        if clean_line:\n",
    "            lines.append(clean_line)\n",
    "    return lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pzndOj8d1Djn"
   },
   "outputs": [],
   "source": [
    "def extract_tables_with_pdfplumber(pdf_path, page_number):\n",
    "    \"\"\"Extract tables from specific page using pdfplumber\"\"\"\n",
    "    print(\"Extract tables from specific page using pdfplumber\")\n",
    "    tables_data = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        # pdfplumber uses 0-based indexing, so subtract 1 from page_number\n",
    "        page = pdf.pages[page_number - 1]\n",
    "        tables = page.extract_tables()\n",
    "\n",
    "        for table_idx, table in enumerate(tables):\n",
    "            print(f\"Table {table_idx + 1} on page {page_number}:\")\n",
    "            table_rows = []\n",
    "            for row in table:\n",
    "                print(row)\n",
    "                # Clean up None values and convert to strings\n",
    "                cleaned_row = [str(cell) if cell is not None else \"\" for cell in row]\n",
    "                table_rows.append(cleaned_row)  # Fixed: was \"table_rows.menaing...append(cleaned_row)\"\n",
    "            tables_data.append(table_rows)\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "        return tables_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "690Aa9BgMVW2"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "DetectorFactory.seed = 0\n",
    "# Unicode range for Bangla: \\u0980-\\u09FF\n",
    "def contains_bangla(text):\n",
    "    return bool(re.search(r'[\\u0980-\\u09FF]', text))\n",
    "\n",
    "def is_mostly_punct(text, threshold=0.6):\n",
    "    if not text:\n",
    "        return True\n",
    "    punct_count = sum(1 for c in text if c in string.punctuation or c.isspace())\n",
    "    return (punct_count / len(text)) > threshold\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main_ocr(page_pixmap, color='g', black=120, psm=6, save_path=None, page_num=None):\n",
    "    img_data = page_pixmap.tobytes(\"png\")\n",
    "    nparr = np.frombuffer(img_data, np.uint8)\n",
    "    img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
    "\n",
    "    gray_image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    if color == 'b':\n",
    "        _, img_processed = cv2.threshold(gray_image, black, 255, cv2.THRESH_BINARY)\n",
    "    else:\n",
    "        img_processed = gray_image\n",
    "\n",
    "    if save_path is not None and page_num is not None:\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        filename = os.path.join(save_path, f'page_{page_num+1}_processed.png')\n",
    "        cv2.imwrite(filename, img_processed)\n",
    "        print(f\"Saved processed image: {filename}\")\n",
    "\n",
    "    text = pytesseract.image_to_string(img_processed, lang='ben', config=f'--oem 3 --psm {psm}')\n",
    "\n",
    "    print(\"text........\")\n",
    "    print(text)\n",
    "    lines = extract_lines_by_line(text)\n",
    "    lines = fix_roman_numerals(lines)\n",
    "\n",
    "\n",
    "    return lines\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def page_has_table(pdf_path, page_number):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        page = pdf.pages[page_number]\n",
    "        tables = page.find_tables()\n",
    "        return len(tables) > 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path, color='g', black=80, save_images=False):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    all_pages = []\n",
    "    save_path = \"processed_images\" if save_images else None\n",
    "\n",
    "    for page_num in range(len(doc)):\n",
    "        page_index = page_num + 1\n",
    "        print(f\"\\nProcessing page {page_index}...\")\n",
    "\n",
    "        # Decide threshold (black value)\n",
    "        if page_index in [2, 41]:\n",
    "            black_val = 120\n",
    "            print(\"Using black=120 for better contrast\")\n",
    "        else:\n",
    "            black_val = black\n",
    "\n",
    "        # force PSM/DPI\n",
    "        if page_index == 41:\n",
    "            dpi = 300\n",
    "            psm = 6\n",
    "            print(\"Page 41 forced — PSM 6, DPI 350\")\n",
    "        elif page_index == 2:\n",
    "            dpi = 350\n",
    "            psm = 6\n",
    "            print(\"Page 2 forced — PSM 6, DPI 350\")\n",
    "        else:\n",
    "            if page_has_table(pdf_path, page_num):\n",
    "                dpi = 300\n",
    "                psm = 11\n",
    "                print(\"Table detected — using PSM 11 and DPI 300\")\n",
    "            else:\n",
    "                dpi = 350\n",
    "                psm = 6\n",
    "                print(\"No table detected — using PSM 6 and DPI 350\")\n",
    "\n",
    "        # OCR\n",
    "        pix = doc.load_page(page_num).get_pixmap(dpi=dpi)\n",
    "        lines = main_ocr(pix, color=color, black=black_val, psm=psm, save_path=save_path, page_num=page_num)\n",
    "\n",
    "        page_data = {\n",
    "            \"page\": page_index,\n",
    "            \"lines\": lines\n",
    "        }\n",
    "\n",
    "        # ADD THIS: For pages 2 and 41, also get OCR from table areas\n",
    "        if page_index in [2, 41]:\n",
    "            print(f\"\\n=== Extracting additional OCR from table areas on page {page_index} ===\")\n",
    "            table_ocr =extract_tables_with_pdfplumber(pdf_path, page_index)\n",
    "            page_data[\"table_ocr\"] = table_ocr\n",
    "\n",
    "            # Also get the structured table data\n",
    "            table_data = extract_tables_with_pdfplumber(pdf_path, page_index)\n",
    "            page_data[\"table_data\"] = table_data\n",
    "\n",
    "        all_pages.append(page_data)\n",
    "\n",
    "    return all_pages\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WQm_f5GV7_wS",
    "outputId": "2bf43230-9035-4238-b0ab-fe71694fc5ab"
   },
   "outputs": [],
   "source": [
    "\n",
    "output_dir = \"raw_data_per_page\"\n",
    "\n",
    "def save_results(data, output_dir):\n",
    "    \"\"\"Save cleaned text to JSON and individual TXT files per page\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save complete data to JSON\n",
    "    json_path = os.path.join(output_dir, \"all_pages.json\")\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # Save each page as separate TXT file\n",
    "    for page in data:\n",
    "        txt_path = os.path.join(output_dir, f\"page_{page['page']}.txt\")\n",
    "        with open(txt_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"\\n\".join(page['lines']))\n",
    "\n",
    "        # For pages 2 and 41, save additional table OCR data if available\n",
    "        if page['page'] in [2, 41]:\n",
    "            if 'table_ocr' in page:\n",
    "                table_ocr_path = os.path.join(output_dir, f\"page_{page['page']}_table_ocr.txt\")\n",
    "                with open(table_ocr_path, 'w', encoding='utf-8') as f:\n",
    "                    # Handle case where table_ocr might contain lists\n",
    "                    table_lines = []\n",
    "                    for row in page['table_ocr']:\n",
    "                        if isinstance(row, list):\n",
    "        # Join cells with tabs to preserve row structure\n",
    "                              table_lines.append(\"\\t\".join(str(cell) if cell else '' for cell in row))\n",
    "                        else:\n",
    "                             table_lines.append(str(row))\n",
    "                    f.write(\"\\n\".join(table_lines))\n",
    "\n",
    "                print(f\"Saved table OCR for page {page['page']}: {table_ocr_path}\")\n",
    "\n",
    "            if 'table_data' in page:\n",
    "                table_json_path = os.path.join(output_dir, f\"page_{page['page']}_table_data.json\")\n",
    "                with open(table_json_path, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(page['table_data'], f, ensure_ascii=False, indent=2)\n",
    "                print(f\"Saved table data for page {page['page']}: {table_json_path}\")\n",
    "\n",
    "    print(f\"Saved results to {output_dir}\")\n",
    "    print(f\"- raw JSON: {json_path}\")\n",
    "    print(f\"- raw TXT files: {len(data)} pages\")\n",
    "\n",
    "raw_data = []\n",
    "pdf_path = \"/content/HSC26-Bangla1st-Paper.pdf\"\n",
    "results = extract_text_from_pdf(pdf_path, color='b', black=80, save_images=True)\n",
    "\n",
    "for page in results:\n",
    "    print(f\"\\nPage {page['page']} - Extracted lines:\")\n",
    "    for line in page[\"lines\"]:\n",
    "        print(line)\n",
    "\n",
    "    # Create the page data dictionary\n",
    "    page_data = {\n",
    "        \"page\": page[\"page\"],\n",
    "        \"lines\": page[\"lines\"]\n",
    "    }\n",
    "\n",
    "    # Add table data for pages 2 and 41\n",
    "    if page[\"page\"] in [2, 41]:\n",
    "        if \"table_ocr\" in page:\n",
    "            page_data[\"table_ocr\"] = page[\"table_ocr\"]\n",
    "        if \"table_data\" in page:\n",
    "            page_data[\"table_data\"] = page[\"table_data\"]\n",
    "\n",
    "    # Append the complete page data to raw_data\n",
    "    raw_data.append(page_data)\n",
    "\n",
    "save_results(raw_data, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bi7U385qBSMY",
    "outputId": "7f631456-01c2-4e4d-9145-9dcf2d6410b0"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "json_path = \"/content/raw_data_per_page/all_pages.json\"\n",
    "\n",
    "# Load existing JSON\n",
    "with open(json_path, 'r', encoding='utf-8') as f:\n",
    "    pages = json.load(f)\n",
    "\n",
    "# Process each page\n",
    "for page in pages:\n",
    "    # Remove table_data key if present\n",
    "    page.pop('table_data', None)\n",
    "\n",
    "    # Merge table_ocr into lines if it exists\n",
    "    if 'table_ocr' in page and isinstance(page['table_ocr'], list):\n",
    "        # Flatten table_ocr rows to strings (handle nested lists)\n",
    "        merged_lines = []\n",
    "        for row in page['table_ocr']:\n",
    "            if isinstance(row, list):\n",
    "                merged_lines.append(\"\\t\".join(str(cell) if cell else '' for cell in row))\n",
    "            else:\n",
    "                merged_lines.append(str(row))\n",
    "\n",
    "        # Append to lines\n",
    "        page['lines'].extend(merged_lines)\n",
    "\n",
    "        # Remove the original table_ocr key\n",
    "        page.pop('table_ocr', None)\n",
    "\n",
    "# Save back the modified JSON (overwrite)\n",
    "with open(json_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(pages, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Updated JSON saved to {json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q_FpCwyNUF6M",
    "outputId": "bcfccf54-3351-47b2-c419-52eec39acff7"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "input_json = \"raw_data_per_page/all_pages.json\"\n",
    "output_cleaned_json = \"clean_data/all_pages_cleaned.json\"\n",
    "output_chunks_json = \"clean_data/faiss_ready_chunks.json\"\n",
    "\n",
    "# Clean each line\n",
    "DASH_CHARS = r\"[-‐‑‒–—―]\"\n",
    "\n",
    "def clean_line(line):\n",
    "    # Remove table borders, pipes, dashes\n",
    "    line = re.sub(r'[|│┃╏═─—_]+', '', line)\n",
    "    line = re.sub(r'[-]{2,}', '', line)\n",
    "    line = re.sub(r'\\[', '', line)\n",
    "    line = re.sub(DASH_CHARS, '', line)\n",
    "    line = re.sub(r'\\.{2,}', '.', line)\n",
    "\n",
    "    # Remove unwanted symbols\n",
    "    line = re.sub(r'[€\\]\\*]', '', line)\n",
    "\n",
    "    # Remove digit sequences that are 4 or more digits long\n",
    "    line = re.sub(r'[০-৯0-9]{4,}', '', line)\n",
    "\n",
    "    # Normalize spaces\n",
    "    line = re.sub(r'\\s+', ' ', line).strip()\n",
    "    line = line.replace('।', '.')\n",
    "\n",
    "    # Remove batch tags and metadata\n",
    "    line = re.sub(r'আনলাইন\\s*ব্যাচ\\s*[\\w০-৯ঃ:]*', '', line)\n",
    "    line = re.sub(r'\\bSL\\s*Ans\\b', '', line, flags=re.IGNORECASE)\n",
    "    line = re.sub(r'\\bPage\\s*\\d+\\b', '', line, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove leading/trailing quotes\n",
    "    line = line.strip(' \"\\'”“‘’')\n",
    "\n",
    "    # Skip empty lines (but **keep single and double digits**)\n",
    "    if not line:\n",
    "        return None\n",
    "\n",
    "    return line\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -------- Step 2: Load and deduplicate --------\n",
    "with open(input_json, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "cleaned_pages = {}\n",
    "\n",
    "for entry in data:\n",
    "    page = entry.get(\"page\")\n",
    "    if page == 1:\n",
    "        continue\n",
    "    lines = entry.get(\"lines\", [])\n",
    "\n",
    "    # --- Special case for page 41: keep only lines from \"SL\" onward ---\n",
    "    if page == 41:\n",
    "        for idx, line in enumerate(lines):\n",
    "            if \"SL\" in line or \"Sl\" in line or \"sl\" in line:\n",
    "                # Keep only lines from here onward\n",
    "                lines = lines[idx:]\n",
    "                break\n",
    "\n",
    "    if page not in cleaned_pages:\n",
    "        cleaned_lines = []\n",
    "        for line in lines:\n",
    "            cleaned = clean_line(line)\n",
    "            if cleaned:\n",
    "                cleaned_lines.append(cleaned)\n",
    "        cleaned_pages[page] = cleaned_lines\n",
    "\n",
    "\n",
    "# Save cleaned per-page JSON\n",
    "final_cleaned = [{\"page\": p, \"lines\": cleaned_pages[p]} for p in sorted(cleaned_pages.keys())]\n",
    "with open(output_cleaned_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(final_cleaned, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Cleaned per-page text saved to: {output_cleaned_json} (Pages: {len(final_cleaned)})\")\n",
    "\n",
    "# -------- Step 3: Chunk for E5 embeddings --------\n",
    "def chunk_for_e5(text, max_chars=1200, overlap=200):\n",
    "    \"\"\"\n",
    "    Break text into chunks (~512 tokens) for multilingual-e5-base.\n",
    "    Adds \"passage:\" prefix for each chunk as required by E5 models.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = min(start + max_chars, len(text))\n",
    "        chunk = text[start:end].strip()\n",
    "        if chunk:\n",
    "            chunks.append(f\"passage: {chunk}\")\n",
    "        start += max_chars - overlap\n",
    "    return chunks\n",
    "\n",
    "all_chunks = []\n",
    "for page_data in final_cleaned:\n",
    "    page_num = page_data[\"page\"]\n",
    "    full_text = \" \".join(page_data[\"lines\"])\n",
    "\n",
    "    chunks = chunk_for_e5(full_text, max_chars=1200, overlap=200)\n",
    "    for i, chunk in enumerate(chunks, start=1):\n",
    "        all_chunks.append({\n",
    "            \"id\": f\"page{page_num}_chunk{i}\",\n",
    "            \"page\": page_num,\n",
    "            \"text\": chunk\n",
    "        })\n",
    "\n",
    "# Save FAISS-ready chunks JSON\n",
    "with open(output_chunks_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_chunks, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"FAISS-ready chunks saved to: {output_chunks_json} (Total chunks: {len(all_chunks)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CxrHv4tKd83f",
    "outputId": "2b833a95-23b4-46aa-c51e-87f674f1dd58"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open(\"clean_data/faiss_ready_chunks.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    chunks = json.load(f)\n",
    "\n",
    "print(f\"Total chunks: {len(chunks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XwS6MljPcKOH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468,
     "referenced_widgets": [
      "0bb5dfdbec5e4c0580b5412dcf72f197",
      "20b4e822bb5a49d4af3b84349bc930fe",
      "8be64079cc864d54b664cdb2285e9277",
      "dac118bc4f144d3286598222166aef3a",
      "1cbd74d4b6364e0683ce10568aa6449f",
      "5000f93bf25c4f05bd4415911c46720b",
      "acd978700d504788a716387c6e582106",
      "dd9e08edcf4c4cdb85f6d26235fa2a9b",
      "8d472e3d2bb4496887acd1dca16a50d7",
      "26c7be00295f40bfb34e3ad913cbd174",
      "0323f4b8926d409684659190cf496170",
      "1d1f9b931907443e9e72b6245917a79e",
      "9c0b41f4a8e5465fba650077a369ace3",
      "205853553f464d85b5930dfc2215e51f",
      "1996cf072b9d409b994ba9a933cc0318",
      "b0682fc5b6264a859512e4ddd8bc4b70",
      "90ce0a229c314c0f89f8886cabef7d6e",
      "0d61ce025a9c4731abdd2cce4eca1b1b",
      "aafbd5c063d84970b2ce6afca38bc9e3",
      "c9f6747e473043deacc88c79e8d73ea6",
      "42a98bf8188448b4afbf4403cdb43739",
      "34927d9df8e146aebfcf523cd3d4c3bf",
      "10dbbd9dc0e047c1ad74963b648a2962",
      "c74ccfa24ae34ce88bd5a2450b965240",
      "40d5cb1e7db246e2a7714067ae5eb99c",
      "c27b9aca7860488db74c848fb490f7db",
      "00ac8db92239417db0fc94fa360c7135",
      "abb4ac9dd8ef431ab32d2a8bc71ed49d",
      "e4a3c5334ac64ef489b8796ecd8a6c5e",
      "4c05a0be80fd4a3388e5bf08b52d708d",
      "d9b2f2be4b754f54aad980b2b881695e",
      "c20f58e8b5e84471936672eb71a0371a",
      "3ae98dc0ac5942eb981afdda8f973675",
      "f61252aa153e4f6a87eacf8a11405ff7",
      "946224004eae4834b0b88360eed6c2a6",
      "dc8f423569834ddc85a48b46cab6bf01",
      "f9a9acded310408186878d86d26ce85e",
      "bdbedb8ff57b4daabcd561c71cbfc635",
      "e555b615206e430a8f866fd57df8484d",
      "db0e952264ee4b68b85e54596db76712",
      "7b85f2d4a83d4bf980a5b389b5ee4fbc",
      "93d45580b40f44f78fc83b9dfc20e0bd",
      "63dfe1f0422543ec9c0dd1371d9e5620",
      "9e16001413ca40709b3c2a765edd0c61",
      "d1f381de562b4082b574bf729dd6a575",
      "d4f813ce5c0d41e8886db92936d1c1e8",
      "b9dff926df1b4dbeb65822e3a9f7c7e6",
      "f6f6015ea0004ff1b071a8a035c58985",
      "8a864f99c5b6490286dfdbf1e03eec1d",
      "04937159b604429490124eae17b9012d",
      "723fa10f4a544f389b5a66cc759a6e98",
      "9bcc0fc9409b406bac46361991648499",
      "c09e06f4b78d445a9bfe068997467683",
      "41971b0ee4b34d428f1a64f5b358cb98",
      "0421f212d0fd445d8df3da050c0a834d",
      "40a47292b30643128ddf06010a3e49d3",
      "d967087eb7da4805b768d57ada9057d3",
      "91f146819b25400eadda3d1a4bf7d8e2",
      "ea3e7e0e7b2a4078aa86092c773d7c33",
      "e82e71009dfa421093fb3078a0335037",
      "3e2bfcf48b3d4d69a7438380a5ed3bc8",
      "cdc8a96e454a40c5a7fbffbe2cc6cd05",
      "31c69eee3da6474196bbac93e03a67d8",
      "32d0a0b54ee94ce69e3275140ec8f5b3",
      "0dbfc686b5fc46d19fb4e57aee088d00",
      "eea925c768564c14a01865d7eb4f6dca",
      "3d9af70dbf894d179f87d5b97afb5146",
      "fe8b5115fd204f959764c102540d8eae",
      "e90711b6aae145ffabc07241f598721a",
      "9e217cb581bb4568aae6c992efb8876c",
      "f0e6f873d0fc4ea79a74ceb4754362e2",
      "aa6dd6632fab414ba8ae6b95211c212a",
      "684bc47adac04f49b36f6e8506161a3b",
      "2eab081b2d5946098a0c396e43e010b6",
      "012c10ee48fb47ca96065c8c703c0cf5",
      "1d7ca3f233e24664bd80812830359a93",
      "ebefeae64fce45bcb3d38b16b85ec8b7",
      "88cfa57394f44b368a7c084fdf29c5f4",
      "ca05e2cfec944cefba097f0fffdaecf0",
      "cb51c08958484846b16997a0a3909f26",
      "803ef4b67a9a4aadbea8bf53a0371924",
      "42859b288a74466f92246cc39fe62427",
      "2d82d9881397455cb3fd0b2a7f70e21d",
      "659172591ade48b7b4d5f814428f3ff0",
      "aac0ce2e007a40f88afd92c12eb55ca4",
      "f621abf315204ada855e9c185e5bc0fe",
      "d15a578967b941b8960ed4fb7c88090f",
      "d6d57c056f694c4c8d41f5bdb9db4bbe",
      "277c8d0f4f834358be9646a12cd94a51",
      "9df3b29ae2c7483ba0c7a8029348a899",
      "d9a9180480814dab9babdf48c490e550",
      "7026438df97449dba07fb539d17267ed",
      "add9b9d6b1bb49cc9ef516b08db5d98b",
      "9d76ddd655d44d88be080be125563661",
      "b979cb98971a4e3f80c729e1c508b141",
      "d5dbb49ddf0c447db3d57e94e5d55553",
      "e293ea3a9a794343ab26aa87b28eeca6",
      "3365fcae410347299946d6cdb476713c",
      "919de6e87b904c278f6562b437db2803",
      "e74f5a0060514e52b8b14eb6bd6193e4",
      "10d412821881425eae5b60f111e418a3",
      "1bbdc390494643c0a4396096ffece4ca",
      "6e3ff68e8a2948e792a8546d812e7517",
      "1a53bfc256754368927b18e4a549463f",
      "7388858d18934ac5992771bb8e60cfad",
      "7f13b5ea9ea74c8db9ed42578ae66c32",
      "657c9d73e2124ca88acd9896f8da6b2e",
      "d72cd26e9e344a50bd9460ffbce04106",
      "f553413837c6413080854ae52ab6acf7",
      "9832a1128d984519888fb144ae8bc976"
     ]
    },
    "id": "vCPFnKTpcSDU",
    "outputId": "177484e2-d3c0-4568-8769-90323e04a915"
   },
   "outputs": [],
   "source": [
    "#Load Embedding Model\n",
    "\n",
    "embedder = SentenceTransformer(\"intfloat/multilingual-e5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5e7WLlNgcU_V"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Preprocess and embed\n",
    "with open(\"clean_data/faiss_ready_chunks.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    chunks = json.load(f)\n",
    "\n",
    "texts = [c[\"text\"] for c in chunks]\n",
    "embeddings = embedder.encode(texts, normalize_embeddings=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHAYTxQtUUZr"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "THjQiNcydHa_",
    "outputId": "e05cb1c1-ca84-4441-846b-271a0b1a0355"
   },
   "outputs": [],
   "source": [
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"Number of chunks (embeddings): {embeddings.shape[0]}\")\n",
    "print(f\"Embedding dimension: {embeddings.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nz-s9vKKeQzB"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "embeddings_np = np.array(embeddings, dtype='float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XpG0T1L0eSrA",
    "outputId": "14de0cdc-10f1-46eb-ba23-15022290bc68"
   },
   "outputs": [],
   "source": [
    "embeddings_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xxFAsS4ZeVYP",
    "outputId": "ec06aa9c-f512-4e6c-ec93-7a8054f2d459"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dim = embeddings_np.shape[1]  # should be 768 for multilingual-e5-base\n",
    "index = faiss.IndexFlatIP(dim)  # Inner product (cosine similarity)\n",
    "index.add(embeddings_np)        # Add all your vectors\n",
    "print(f\"FAISS index built with {index.ntotal} vectors\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369,
     "referenced_widgets": [
      "f16af106b5ec4b52a6a096dfc45db291",
      "de8a63f8461649af8dacdc6472b0b373",
      "bf8458e2764a4fc996cb8e230b6efe75",
      "9306abb6dbb54110b8b65da9028e04ef",
      "214f5222b95f402dacb5deefbb8c32e9",
      "af0767848a794da3b6040dc4481ea19e",
      "10d5061800314ba1a7118008060cab53",
      "44632023fad34f1cab4f39f5e2ea9075",
      "ca2e81c9b4e1471e84793dc27222461f",
      "5e721d19e71641ca8e92ae37bab347c0",
      "026a864d7607415b81f41474ce7169a2",
      "bb74858ce16d4184b92afc82fdd1290d",
      "53851a29b25f442d89cfe916ad0d9d40",
      "a2d3aee731f2493397ca96d4ad329b70",
      "5bfe377ac1a0485fbab91184608ccc2f",
      "98dd470527344af6a8312d8527b15169",
      "e87796d16c8b44368ebf1db91559c9fc",
      "acdccc8eb0ea43e187f1f35083aa2449",
      "b30657c83058466b9cef61e821e283dc",
      "2a6d89916d0a4e148355d8be1af12b9f",
      "a466bb6df2124cc48ee326c13029fbb8",
      "10f93425febd43a1acca70dbc92a228d",
      "c2a696af094f4c56b773513388133574",
      "e00e7792c9a840e982ac15727db890ef",
      "5fb99e84096241e9bb3db85f12b19296",
      "8bdca2e386754f7cb69abc2b64cc8f42",
      "305dbf90be864fff999b527c0b404953",
      "db8ba1a396654da5bfec1e156021b3d2",
      "a783a6b5416e4d7eaea9b5735c6fbeea",
      "2b5359bc44424ca48295bbc4a4919503",
      "e66d62d2762e4b8ab732dfcf17e6e231",
      "cf0b767742a9412988e6b1c123527361",
      "dee8176bf0fd4b6d8f9e887803fc0d2c",
      "92fac8ad73364ea49af06964f4c4bdf2",
      "fd573ec4c4764e96aaf26fab1c963fda",
      "53813556d4f04a318c877fdd7c10f5c4",
      "eadb1a5e61204fc8aac11df54daa6790",
      "c7b0093578064b189638cab7b57b9181",
      "ebe34ad15e42446fa41dec6305d1b591",
      "1fac884d9d7e405fbd2f7948032432ee",
      "7e83771bdab54d1e9de0bf675da9523a",
      "149b55afb3b949398d03d8492e425243",
      "495bf207befb47269668ef4d1ff67111",
      "e0e54ba1e9ad4f9d91413008c888003e",
      "8474397861fe4ceaab4de1d842d6dba7",
      "5e94e3d2d01e45c6b657f652b75c51e1",
      "15ae95a7172c4df7835691576576cf56",
      "2fdb1d32017e4fd4907bb82af33da792",
      "5ca5c38d394045889ac9deb554b8b9ad",
      "1a811245bfdc489aa879a0c976b28ae8",
      "e1f5ad9913474913b79604527eeec598",
      "f53d725a1ec2434683ddbdd37a17224b",
      "1ff02efa504c49778bec82d839ab1806",
      "a70d6cb7e60146f688204f492820df0b",
      "cc6ac9ddd73c4b9f85e94eee5f5da57a",
      "97fb09cb7d5d4d36a4b4b50e2f01cde6",
      "e6c3d62095fb448d9098190bd5b277e3",
      "187da3058e974cb889f2f77e10aee47a",
      "d62d500485fb47fb8a502e8733034f68",
      "b97e47a63b4c43a7a42b8c7ee337b4ea",
      "be7eccd22a314d3e912badcf4d378b51",
      "38a627f0f2d14dae957dfc1802d250be",
      "0c89555f0b9c487c8b83c1b4cc7b710a",
      "a7e4805108f94ca1b4cd9c87bd8b4765",
      "f5922761245e44998f9883515a2b65e2",
      "4b7b86da460b44eaa87aad1adb3efc6b",
      "2ef5075ff62e4a7e87362e84040a744d",
      "1145383fd3d54d9ebb42ed7f48bacf65",
      "796fa90145d04d5a9c8ec7e806213a9e",
      "2a352c02adec4e51b94c920b2b2ebc58",
      "13de840750f647b886ca62518fa27c65",
      "cf4fef1d5ab04308a0a77366479d85bd",
      "911198f17b1c4c5098854a26d18593a0",
      "0869c82658e74858a709bbb4a0f06602",
      "cbd6ac7d5bdd4841b13e94e77c621f49",
      "c220248135844242b82b015538712026",
      "07a75906fe4b443f9e8d73a06befffc7",
      "daa71d28a9a74e928ac2a139d5dd24c5",
      "33938f64cf154a6198741b94835a2aab",
      "da496e4d921044c8b5690e9de52298d4",
      "b4a6762be57245a9a03415347675eb70",
      "00420fc8d2624791ab707b0cd2890e02",
      "d1aa26f29e214aad8cd1991ddb508c8f",
      "14022bf3d1e1436b8ba43b8503526bc3",
      "21af692735b1443b807b003387ac732d",
      "c670d83e6daa4065ae334390a831d3d7",
      "438e1edf175f4e11817431f0a8c1aaff",
      "f24b1b11f5fd4e85bb02364b7ed7447d",
      "cd5c55fd92744e148e41033478de3c34",
      "6c0ff7ab350e4630910fdecec47ff64c",
      "344cbaa188f84d738f354eeaaa051d34",
      "37ef4e04cc224d03b7b2faa202979ca4",
      "f8bd90c853b344f9b5db51837f52585c",
      "e659e6fd38324c9f8e34acfe32ac238c",
      "96e1cd7fa84d48a1a2b368335e18593b",
      "b6e23d4d87f845b8ba4eac4389039f95",
      "56aae84369f84136ab075e1a53ec2186",
      "3ccf9de6ced84081b57e32919972bfb4",
      "869451750d9f426d874718940d6c2fe9",
      "c360fc7c3396412fab9d5f789617f48a",
      "e0a6690afa374aa399f9f2658a47aaac",
      "ddeeeb95f1ad424bbb256f300836fc0c",
      "b48309ee9a2546bc8b2aecf49b4f11b4",
      "6e7bb9f2536745f8bdb71046e462c6e7",
      "f75e885881824d399b991d764f894f07",
      "da4df46665e14d689f920bbc908ce9f3",
      "afc33d1b741046b196479465e73f171c",
      "49526b79934d4697897c18dc3e5fd684",
      "fac575bd9a654b578cfbfbe4d8095733",
      "4e9a07245ddf46e4906ea29f95eb449f",
      "075bdf1d317a4d1197fcc4656d34d92a",
      "4b82f0214b49427ba993ae269caf5cf6",
      "1184dd46681c410b9e3c2d98f29b0aee",
      "a17f7422d10a463f9c20d2c477648438",
      "0e6987acbbdb422ab3c5a3a7547012a7",
      "d70c8e23c682433bbf0892c1a1324d06",
      "21b787c168594018abdbd8ad367e1968",
      "33a519a1432443c1b42aa1b82413b3cc",
      "f0d5ad22b5f74b7a98d629e4b8871dd4",
      "194bc58299cf463c87e7eb622ff4a705",
      "2aa65602506f41bfaf32e4a2a9e0d694"
     ]
    },
    "id": "2uJZYEIfe8aL",
    "outputId": "2a8fd6e8-9574-45c9-cbc8-4e2c0db7c07e"
   },
   "outputs": [],
   "source": [
    "gen_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-xl\")\n",
    "gen_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4evtMqj2Q9Ei"
   },
   "outputs": [],
   "source": [
    "def build_prompt_from_query(query, embedder, index, chunks, top_k=1):\n",
    "    # Embed the query\n",
    "    query_emb = embedder.encode([query], normalize_embeddings=True)\n",
    "    query_emb = np.array(query_emb, dtype='float32')\n",
    "\n",
    "    # Search top_k relevant chunks\n",
    "    scores, indices = index.search(query_emb, top_k)\n",
    "    indices = indices[0]\n",
    "    scores = scores[0]\n",
    "\n",
    "    # Filter out invalid indices if any\n",
    "    valid_indices = [i for i in indices if i < len(chunks)]\n",
    "\n",
    "    # Retrieve the text chunks\n",
    "    retrieved_docs = [chunks[idx]['text'] for idx in valid_indices]\n",
    "\n",
    "    # Print retrieved chunks for debugging\n",
    "    print(\"Retrieved chunks for prompt:\")\n",
    "    for i, text in enumerate(retrieved_docs, 1):\n",
    "        print(f\"Chunk {i}:\")\n",
    "        split_text = re.split(r'[।?\\.]', text)\n",
    "        for sentence in split_text:\n",
    "            sentence = sentence.strip()\n",
    "            if sentence:\n",
    "                print(sentence)\n",
    "        print('-' * 50)\n",
    "\n",
    "    # Build the final prompt\n",
    "    context = \"\\n\".join(retrieved_docs)\n",
    "    prompt = (\n",
    "        f\"প্রশ্ন: {query}\\n\"\n",
    "        f\"নীচের তথ্য থেকে প্রশ্নের সঠিক উত্তর খুঁজে বের করুন।\\n\"\n",
    "        f\"উত্তরটি এমন অংশ থেকে নিন যেটাতে 'SL Ans', 'উত্তর', বা সমমানের অন্য কোনো চিহ্ন রয়েছে।\\n\"\n",
    "        f\"যদি স্পষ্ট উত্তর না পাওয়া যায়, তাহলে 'উত্তর পাওয়া যায়নি' লিখুন।\\n\"\n",
    "        f\"উত্তর বাংলায় লিখুন:\\n\\n\"\n",
    "        f\"{context}\\n\"\n",
    "    )\n",
    "\n",
    "    # Return both: the prompt and the retrieved chunks\n",
    "    return prompt, retrieved_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r1LFLTvUUocZ",
    "outputId": "18462941-bfe6-4f54-fd3e-1ea049c722dd"
   },
   "outputs": [],
   "source": [
    "query = \"কে পণ করিয়াছে বিবাহ করিবে না।\"\n",
    "prompt = build_prompt_from_query(query, embedder, index, chunks, top_k=1)\n",
    "print(\"Prompt for model:\\n\", prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "epNoOGFFSIrn",
    "outputId": "2fac729e-ee82-4d49-ecc9-f91c3ada3afe"
   },
   "outputs": [],
   "source": [
    "query = \"অনুপমের বাবা কী করে জীবিকা নির্বাহ করতেন?\"\n",
    "prompt = build_prompt_from_query(query, embedder, index, chunks, top_k=1)\n",
    "print(\"Prompt for model:\\n\", prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvoBGTuASDxK",
    "outputId": "49474ed0-e59a-485b-aaec-c7723ece74bb"
   },
   "outputs": [],
   "source": [
    "query = \"অনুপমের ভাষায় সুপুরুষ কাকে বলা হয়েছে?\"\n",
    "prompt = build_prompt_from_query(query, embedder, index, chunks, top_k=1)\n",
    "print(\"Prompt for model:\\n\", prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TyK7fq69Q_VP",
    "outputId": "c8b3b114-3223-47ca-8ff8-338136704af0"
   },
   "outputs": [],
   "source": [
    "query = \"বিয়ের সময় কল্যাণীর প্রকৃত বয়স কত ছিল?\"\n",
    "prompt = build_prompt_from_query(query, embedder, index, chunks, top_k=1)\n",
    "print(\"Prompt for model:\\n\", prompt)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
